{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crf模型定义与训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 配置模板\n",
    "UNIGRAM_PATTERNS = [[-2], [-1], [0], [1], [2], [-2, -1], [-1, 0], [-1, 1], [0, 1], [2, 2]]\n",
    "BIGRAM_PATTERNS = [[-2], [-1], [0], [1], [2], [-2, -1], [-1, 0], [-1, 1], [0, 1], [2, 2]]\n",
    "TAGS = [\"B\", \"M\", \"E\", \"S\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 特征权重\n",
    "feature_weights = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    buffer_text, buffer_label = \"\", \"\"\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if buffer_text and buffer_label:\n",
    "                    sentences.append(buffer_text)\n",
    "                    labels.append(buffer_label)\n",
    "                buffer_text, buffer_label = \"\", \"\"\n",
    "            else:\n",
    "                char, tag = line.split(\"\\t\")\n",
    "                buffer_text += char\n",
    "                buffer_label += tag\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crf方法定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_key(template, template_id, text, idx, tag_sequence):\n",
    "    key = str(template_id)\n",
    "    for offset in template:\n",
    "        pos = idx + offset\n",
    "        key += text[pos] if 0 <= pos < len(text) else \" \"\n",
    "    return f\"{key}/{tag_sequence}\"\n",
    "\n",
    "def compute_score(text, pos, tag_or_pair, patterns, is_bigram=False):\n",
    "    total = 0\n",
    "    for i, template in enumerate(patterns):\n",
    "        key = build_feature_key(template, i, text, pos, tag_or_pair)\n",
    "        total += feature_weights.get(key, 0)\n",
    "    return total\n",
    "\n",
    "def tag_index_to_label(idx):\n",
    "    return TAGS[idx]\n",
    "\n",
    "def tag_label_to_index(label):\n",
    "    return TAGS.index(label)\n",
    "\n",
    "def viterbi_decode(text):\n",
    "    n = len(text)\n",
    "    dp = [[float('-inf')] * n for _ in range(4)]\n",
    "    paths = [[\"\"] * n for _ in range(4)]\n",
    "\n",
    "    for tag_idx in range(4):\n",
    "        tag = tag_index_to_label(tag_idx)\n",
    "        dp[tag_idx][0] = compute_score(text, 0, tag, UNIGRAM_PATTERNS) + \\\n",
    "                         compute_score(text, 0, \" \" + tag, BIGRAM_PATTERNS)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        for curr_idx in range(4):\n",
    "            curr_tag = tag_index_to_label(curr_idx)\n",
    "            best_score, best_prev = float('-inf'), 0\n",
    "            for prev_idx in range(4):\n",
    "                prev_tag = tag_index_to_label(prev_idx)\n",
    "                score = dp[prev_idx][i - 1] + \\\n",
    "                        compute_score(text, i, curr_tag, UNIGRAM_PATTERNS) + \\\n",
    "                        compute_score(text, i, prev_tag + curr_tag, BIGRAM_PATTERNS)\n",
    "                if score > best_score:\n",
    "                    best_score, best_prev = score, prev_idx\n",
    "            dp[curr_idx][i] = best_score\n",
    "            paths[curr_idx][i] = tag_index_to_label(best_prev)\n",
    "\n",
    "    best_final = max(range(4), key=lambda i: dp[i][-1])\n",
    "    result = [\"\"] * n\n",
    "    result[-1] = tag_index_to_label(best_final)\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        result[i] = paths[tag_label_to_index(result[i + 1])][i + 1]\n",
    "\n",
    "    return \"\".join(result)\n",
    "\n",
    "def update_feature_weights(text, gold_tags):\n",
    "    pred_tags = viterbi_decode(text)\n",
    "    for i, (pred, truth) in enumerate(zip(pred_tags, gold_tags)):\n",
    "        if pred != truth:\n",
    "            for j, template in enumerate(UNIGRAM_PATTERNS):\n",
    "                for tag in [pred, truth]:\n",
    "                    delta = 1 if tag == truth else -1\n",
    "                    key = build_feature_key(template, j, text, i, tag)\n",
    "                    feature_weights[key] = feature_weights.get(key, 0) + delta\n",
    "\n",
    "            prev_pred = pred_tags[i - 1] if i > 0 else \" \"\n",
    "            prev_truth = gold_tags[i - 1] if i > 0 else \" \"\n",
    "            for j, template in enumerate(BIGRAM_PATTERNS):\n",
    "                for pair, delta in [(prev_pred + pred, -1), (prev_truth + truth, 1)]:\n",
    "                    key = build_feature_key(template, j, text, i, pair)\n",
    "                    feature_weights[key] = feature_weights.get(key, 0) + delta\n",
    "\n",
    "def count_errors(pred, truth):\n",
    "    return sum(p != t for p, t in zip(pred, truth))\n",
    "\n",
    "def count_matches(pred, truth):\n",
    "    return sum(p == t for p, t in zip(pred, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助评估方法定义，包括准确率，精确率，召回率和F值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(text, tags):\n",
    "    words = []\n",
    "    word = ''\n",
    "    for ch, tag in zip(text, tags):\n",
    "        word += ch\n",
    "        if tag in ('S', 'E'):\n",
    "            words.append(word)\n",
    "            word = ''\n",
    "    if word:\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "def calc_metrics(pred_tags_list, gold_tags_list, sentence_list):\n",
    "    total_chars, correct_chars = 0, 0\n",
    "    total_pred_words, total_gold_words, correct_words = 0, 0, 0\n",
    "\n",
    "    for pred_tags, gold_tags, sentence in zip(pred_tags_list, gold_tags_list, sentence_list):\n",
    "        total_chars += len(sentence)\n",
    "        correct_chars += sum(p == g for p, g in zip(pred_tags, gold_tags))\n",
    "\n",
    "        pred_words = set(_span_indices(split_words(sentence, pred_tags)))\n",
    "        gold_words = set(_span_indices(split_words(sentence, gold_tags)))\n",
    "\n",
    "        total_pred_words += len(pred_words)\n",
    "        total_gold_words += len(gold_words)\n",
    "        correct_words += len(pred_words & gold_words)\n",
    "\n",
    "    accuracy = correct_chars / total_chars\n",
    "    precision = correct_words / total_pred_words if total_pred_words else 0\n",
    "    recall = correct_words / total_gold_words if total_gold_words else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def _span_indices(words):\n",
    "    spans = []\n",
    "    pos = 0\n",
    "    for word in words:\n",
    "        spans.append((pos, pos + len(word)))\n",
    "        pos += len(word)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_path, model_path, epochs=5):\n",
    "    sentences, labels = load_training_data(train_path)\n",
    "    split = int(0.8 * len(sentences))\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_errors = 0\n",
    "        pred_tags_list, gold_tags_list = [], []\n",
    "\n",
    "        # 训练部分\n",
    "        for i in range(split):\n",
    "            s, l = sentences[i], labels[i]\n",
    "            update_feature_weights(s, l)\n",
    "            pred_tags = viterbi_decode(s)\n",
    "            pred_tags_list.append(pred_tags)\n",
    "            gold_tags_list.append(l)\n",
    "            total_errors += count_errors(pred_tags, l)\n",
    "\n",
    "        acc, pre, rec, f1 = calc_metrics(pred_tags_list, gold_tags_list, sentences[:split])\n",
    "        print(f\"Epoch {epoch} - Train\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {pre:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "        # 测试部分\n",
    "        pred_tags_list, gold_tags_list = [], []\n",
    "        for i in range(split, len(sentences)):\n",
    "            s, l = sentences[i], labels[i]\n",
    "            pred = viterbi_decode(s)\n",
    "            pred_tags_list.append(pred)\n",
    "            gold_tags_list.append(l)\n",
    "\n",
    "        acc, pre, rec, f1 = calc_metrics(pred_tags_list, gold_tags_list, sentences[split:])\n",
    "        print(f\"Epoch {epoch} - Test\")\n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {pre:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "        torch.save({\n",
    "            'feature_weights': feature_weights,\n",
    "            'unigram_patterns': UNIGRAM_PATTERNS,\n",
    "            'bigram_patterns': BIGRAM_PATTERNS\n",
    "        }, model_path)\n",
    "    print(f\"模型 {model_path} 训练完成\\n\")\n",
    "\n",
    "\n",
    "def predict(sentence, model_path):\n",
    "    model = torch.load(model_path, map_location=DEVICE)\n",
    "    global feature_weights, UNIGRAM_PATTERNS, BIGRAM_PATTERNS\n",
    "    feature_weights = model['feature_weights']\n",
    "    UNIGRAM_PATTERNS = model['unigram_patterns']\n",
    "    BIGRAM_PATTERNS = model['bigram_patterns']\n",
    "\n",
    "    tags = viterbi_decode(sentence)\n",
    "\n",
    "    if tags[-1] in ('B', 'M'):\n",
    "        tags = tags[:-1] + ('E' if tags[-2] in ('B', 'M') else 'S',)\n",
    "\n",
    "    segmented = ''\n",
    "    for i, ch in enumerate(sentence):\n",
    "        segmented += ch\n",
    "        if tags[i] in ('S', 'E') and i != len(sentence) - 1:\n",
    "            segmented += '|'\n",
    "\n",
    "    print(\"原始语句：\", sentence)\n",
    "    print(\"BMES标签：\", ' '.join(tags))\n",
    "    print(\"分词结果：\", segmented)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train\n",
      "  Accuracy:  0.9905\n",
      "  Precision: 0.9884\n",
      "  Recall:    0.9886\n",
      "  F1 Score:  0.9885\n",
      "Epoch 1 - Test\n",
      "  Accuracy:  0.9191\n",
      "  Precision: 0.9065\n",
      "  Recall:    0.9034\n",
      "  F1 Score:  0.9049\n",
      "Epoch 2 - Train\n",
      "  Accuracy:  0.9973\n",
      "  Precision: 0.9967\n",
      "  Recall:    0.9968\n",
      "  F1 Score:  0.9968\n",
      "Epoch 2 - Test\n",
      "  Accuracy:  0.9345\n",
      "  Precision: 0.9266\n",
      "  Recall:    0.9196\n",
      "  F1 Score:  0.9231\n",
      "Epoch 3 - Train\n",
      "  Accuracy:  0.9988\n",
      "  Precision: 0.9986\n",
      "  Recall:    0.9986\n",
      "  F1 Score:  0.9986\n",
      "Epoch 3 - Test\n",
      "  Accuracy:  0.9425\n",
      "  Precision: 0.9360\n",
      "  Recall:    0.9288\n",
      "  F1 Score:  0.9324\n",
      "Epoch 4 - Train\n",
      "  Accuracy:  0.9992\n",
      "  Precision: 0.9991\n",
      "  Recall:    0.9990\n",
      "  F1 Score:  0.9990\n",
      "Epoch 4 - Test\n",
      "  Accuracy:  0.9443\n",
      "  Precision: 0.9388\n",
      "  Recall:    0.9302\n",
      "  F1 Score:  0.9345\n",
      "Epoch 5 - Train\n",
      "  Accuracy:  0.9996\n",
      "  Precision: 0.9995\n",
      "  Recall:    0.9995\n",
      "  F1 Score:  0.9995\n",
      "Epoch 5 - Test\n",
      "  Accuracy:  0.9477\n",
      "  Precision: 0.9418\n",
      "  Recall:    0.9351\n",
      "  F1 Score:  0.9384\n",
      "模型 crf_model.pt 训练完成\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"BMES_corpus.txt\"\n",
    "model_path = \"crf_model.pt\"\n",
    "epochs = 5\n",
    "\n",
    "train(train_path, model_path, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始语句： 他说的的确有道理，他说的确实是对的\n",
      "BMES标签： S S S B E S B E S S S S B E S S S\n",
      "分词结果： 他|说|的|的确|有|道理|，|他|说|的|确实|是|对|的\n",
      "\n",
      "原始语句： 这是一项有意义的研究生活动，用来研究生活中的科学\n",
      "BMES标签： S S S S S B E S B M E B E S S S B E B E S S B E\n",
      "分词结果： 这|是|一|项|有|意义|的|研究生|活动|，|用|来|研究|生活|中|的|科学\n",
      "\n",
      "原始语句： 4月1日出版的第7期《求是》杂志发表总书记重要文章《朝着建成科技强国的宏伟目标奋勇前进》。如何一步一个脚印把建成科技强国的战略目标变为现实？习总书记这样指引方向\n",
      "BMES标签： B E B E B E S B E S S B E S B E B E B M E B E B E S B E B E B E B E S B E B E B E B E S S B E B M M M M E S B E B E B E S B E B E B E B E S S B M E B E B E B E\n",
      "分词结果： 4月|1日|出版|的|第7|期|《|求是|》|杂志|发表|总书记|重要|文章|《|朝着|建成|科技|强国|的|宏伟|目标|奋勇|前进|》|。|如何|一步一个脚印|把|建成|科技|强国|的|战略|目标|变为|现实|？|习|总书记|这样|指引|方向\n",
      "\n",
      "原始语句： 关于紧急开发的中缅英翻译系统，该系统由国家应急语言服务团秘书处和北京语言大学迅速组建的语言服务支持团队，在仅仅七小时内利用AI开发完成。\n",
      "BMES标签： B E B E B E S B M E B E B E S S B E S B E B E B E B M E B M E S B E B E B E B E B E S B E B E B E B E S S B E S B E S B E B E B E B E S\n",
      "分词结果： 关于|紧急|开发|的|中缅英|翻译|系统|，|该|系统|由|国家|应急|语言|服务团|秘书处|和|北京|语言|大学|迅速|组建|的|语言|服务|支持|团队|，|在|仅仅|七|小时|内|利用|AI|开发|完成|。\n",
      "\n",
      "原始语句： 他严格要求自己，从一个科举出身的进士成为一个伟大的民主主义者、一个共产主义战士。\n",
      "BMES标签： S B E B E B E S S B E B E B E S B E B E B E B E S B M M M E S B E B M M E B E S\n",
      "分词结果： 他|严格|要求|自己|，|从|一个|科举|出身|的|进士|成为|一个|伟大|的|民主主义者|、|一个|共产主义|战士|。\n",
      "\n",
      "原始语句： 但就在那一年，他为当时还默默无闻的青年选手奥尔同波士顿棕熊队谈判达成了一项年薪七万美元的合同。\n",
      "BMES标签： S S S S S S S S S B E S B M M E S B E B E B E S B M E B M E B E B E S S S B E B E B E S B E S\n",
      "分词结果： 但|就|在|那|一|年|，|他|为|当时|还|默默无闻|的|青年|选手|奥尔|同|波士顿|棕熊队|谈判|达成|了|一|项|年薪|七万|美元|的|合同|。\n",
      "\n",
      "原始语句： 行行出状元，干一行，行一行\n",
      "BMES标签： B E S B E S S B E S S S S\n",
      "分词结果： 行行|出|状元|，|干|一行|，|行|一|行\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"crf_model.pt\"  # 训练好的模型路径\n",
    "sentence = \"他说的的确有道理，他说的确实是对的\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"这是一项有意义的研究生活动，用来研究生活中的科学\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"4月1日出版的第7期《求是》杂志发表总书记重要文章《朝着建成科技强国的宏伟目标奋勇前进》。如何一步一个脚印把建成科技强国的战略目标变为现实？习总书记这样指引方向\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"关于紧急开发的中缅英翻译系统，该系统由国家应急语言服务团秘书处和北京语言大学迅速组建的语言服务支持团队，在仅仅七小时内利用AI开发完成。\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"他严格要求自己，从一个科举出身的进士成为一个伟大的民主主义者、一个共产主义战士。\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"但就在那一年，他为当时还默默无闻的青年选手奥尔同波士顿棕熊队谈判达成了一项年薪七万美元的合同。\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"行行出状元，干一行，行一行\"\n",
    "predict(sentence, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始语句： 明明明明明白白白喜欢他可是他就是不说\n",
      "BMES标签： B E B E B E S E S E S B E S S S S S\n",
      "分词结果： 明明|明明|明白|白|白|喜|欢|他|可是|他|就|是|不|说\n",
      "\n",
      "原始语句： 重庆市长江边的景色迷人，让人流连忘返。\n",
      "BMES标签： B M E B E S S B E B E S S S B M M E S\n",
      "分词结果： 重庆市|长江|边|的|景色|迷人|，|让|人|流连忘返|。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"crf_model.pt\"  # 训练好的模型路径\n",
    "sentence = \"明明明明明白白白喜欢他可是他就是不说\"\n",
    "predict(sentence, model_path)\n",
    "sentence = \"重庆市长江边的景色迷人，让人流连忘返。\"\n",
    "predict(sentence, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoshino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
